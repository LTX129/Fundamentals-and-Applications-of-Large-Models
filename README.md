

# Transformer Summarization (Gigaword Subset)

æœ¬é¡¹ç›®ä¸ºã€ŠFundamentals and Applications of Large Modelsã€‹è¯¾ç¨‹æœŸä¸­å®éªŒï¼Œå®ç°äº†ä¸€ä¸ªä»é›¶æ­å»ºçš„ **Encoder-Decoder Transformer**ï¼Œå¹¶åœ¨ **Gigaword æ ‡é¢˜ç”Ÿæˆä»»åŠ¡**ä¸Šè¿›è¡Œäº†è®­ç»ƒåŠæ¶ˆèå®éªŒã€‚æ¨¡å‹åŒ…å«ï¼š

- Multi-Head Self-Attention
- Position-wise Feed Forward Network
- æ®‹å·®è¿æ¥ + LayerNorm
- å¯é€‰å…±äº«è¯åµŒå…¥
- Learned Positional Encoding
- Greedy / Beam Search è§£ç 
- Noam (Warmup) å­¦ä¹ ç‡è°ƒåº¦
- Label Smoothingã€AMPã€æ¢¯åº¦è£å‰ª

å¹¶æ”¯æŒ **DUC2004 å¤šå‚è€ƒ BLEU/ROUGE è¯„æµ‹**ã€‚

---

## ğŸ“¦ ç¯å¢ƒè¦æ±‚

é¡¹ç›®åŸºäº minicondaï¼Œå»ºè®®ä½¿ç”¨ GPU è®­ç»ƒã€‚

| ç»„ä»¶ | æ¨èç‰ˆæœ¬ |
|---|---|
| Python | â‰¥ 3.9 |
| PyTorch | â‰¥ 2.1 (æ”¯æŒ CUDA) |
| CUDA | â‰¥ 11.7 |
| GPU | è‡³å°‘ 8GB æ˜¾å­˜ï¼ˆ16GB+ æ¨èï¼‰ |

åˆ›å»ºè™šæ‹Ÿç¯å¢ƒç¤ºä¾‹ï¼š

```bash
git clone 
conda create -n largemodel python=3.10 -y
conda activate largemodel
cd largemodel
pip install -r requirements.txt 
```
## ğŸ“‚ ä»£ç ç»“æ„
```
largemodel/
â”œâ”€â”€ checkpoints/                      # è®­ç»ƒ/è¯„æµ‹ä¿å­˜çš„æƒé‡ï¼ˆbest.pt / last.pt ç­‰ï¼‰
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ sumdata/
â”‚       â”œâ”€â”€ DUC2003/
â”‚       â”œâ”€â”€ DUC2004/                  # input.txt + task1_ref*.txtï¼ˆå¤šå‚è€ƒè¯„æµ‹ï¼‰
â”‚       â”œâ”€â”€ Giga/
â”‚       â””â”€â”€ train/                    # train.article.txt / train.title.txt / â€¦
â”‚   â”œâ”€â”€ metafile.yaml
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ outputs/                      # è®­ç»ƒç”Ÿæˆçš„æ›²çº¿å›¾ã€é¢„æµ‹ä¸ä¸­é—´ç»“æœï¼ˆwork_dir å¯æŒ‡å‘æ­¤å¤„ï¼‰
â”‚   â””â”€â”€ result_terminal collect.txt   # ç»ˆç«¯æ—¥å¿—æ±‡æ€»
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ env.sh                        # ç¯å¢ƒå˜é‡ï¼ˆè®¾ç½® PYTHONPATH ç­‰ï¼‰
â”‚   â”œâ”€â”€ run.sh                        # baseline è®­ç»ƒè„šæœ¬ï¼ˆå«å›ºå®šéšæœºç§å­ï¼‰
â”‚   â”œâ”€â”€ evaluate.sh                   # DUC2004 è¯„æµ‹è„šæœ¬
â”‚   â”œâ”€â”€ run_ablation.sh               # ä¸€é”®è¿è¡Œä¸‰ç»„æ¶ˆèå®éªŒ
â”‚   â”œâ”€â”€ plot_ablation.py              # ç”Ÿæˆæ¶ˆèå¯è§†åŒ–ï¼ˆå¦‚ rouge1_vs_epoch.png ç­‰ï¼‰
â”‚   â””â”€â”€ collect_results.py            # æ”¶é›†/æ•´ç†å®éªŒæ—¥å¿—ä¸æŒ‡æ ‡
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                       # è®­ç»ƒ / éªŒè¯ / ä¿å­˜åŠ è½½
â”‚   â”œâ”€â”€ transformer.py                # Encoder-Decoder & Greedy/Beam
â”‚   â”œâ”€â”€ layers.py                     # MHA / FFN / LayerNorm / PosEnc
â”‚   â”œâ”€â”€ tokenizer.py                  # è¯è¡¨ & ç¼–ç è§£ç  & ç‰¹æ®Šç¬¦å·
â”‚   â”œâ”€â”€ gigaword.py                   # æ•°æ®è¯»å– & DUC2004 å¤šå‚è€ƒè¯„æµ‹
â”‚   â”œâ”€â”€ schedule.py                   # Noam å­¦ä¹ ç‡è°ƒåº¦
â”‚   â”œâ”€â”€ metering.py                   # æ»‘åŠ¨å‡å€¼ / å¯è§†åŒ–è¾…åŠ©
â”‚   â””â”€â”€ bleu_rouge.py                 # BLEU / ROUGEï¼ˆå¤šå‚è€ƒï¼‰
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```
â¸»
## ğŸ¯ å›ºå®šéšæœºç§å­ï¼ˆå¯å¤ç°å®éªŒç»“æœï¼‰
ä½œä¸šä¸­æ²¡æœ‰è®¾ç½®éšæœºç§å­ï¼Œå¦‚æœ‰éœ€è¦å¯ä»¥åœ¨ main.py é¡¶éƒ¨åŠ å…¥ï¼š

```bash
import random, torch, numpy as np
def set_seed(seed=3407):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
set_seed(3407)
```
â¸»
## ğŸš€ è®­ç»ƒï¼ˆBaselineï¼‰
ä½¿ç”¨ scripts é‡Œé¢çš„ run.sh è„šæœ¬å¼€å§‹è®­ç»ƒ
```bash
chmod +x scripts/run.sh
bash scripts/run.sh
```

â¸»

## ğŸ§ª DUC2004 å¤šå‚è€ƒè¯„æµ‹
è®­ç»ƒè¿‡ç¨‹è‡ªå¸¦ eval ç¯èŠ‚ï¼Œå¦‚æœæƒ³å•ç‹¬è¿›è¡Œ evalï¼Œå¯ä½¿ç”¨ scripts é‡Œçš„ evaluate.sh è¿›è¡Œè¯„ä¼°
```bash
chmod +x scripts/evaluate.sh

# ç›´æ¥è¯„æµ‹ï¼ˆç”¨é»˜è®¤è·¯å¾„ï¼‰
scripts/evaluate.sh

# è¦†ç›–æŸäº›å‚æ•°ï¼ˆæ¯”å¦‚æ¢è§£ç æ›´çŸ­çš„æ ‡é¢˜ï¼‰
scripts/evaluate.sh --eos_bias 0.8 --max_len 18
```
â¸»
## ğŸ”¬ æ¶ˆèå®éªŒ
å¯ä½¿ç”¨ scripts é‡Œçš„ run_ablation.sh è¿›è¡Œæ¶ˆèå®éªŒ
```bash
# èµ‹æƒ
chmod +x scripts/run_ablation.sh
# è¿è¡Œæ‰€æœ‰æ¶ˆè
bash scripts/run_ablation.sh
# æ±‡æ€»ç»“æœ â†’ CSV/Markdown
python scripts/collect_results.py /root/workspace/tmp/largemodel/outputs
# ç”»å¯¹æ¯”æ›²çº¿ï¼ˆå¯é€‰ï¼‰
python scripts/plot_ablation.py /root/workspace/tmp/largemodel/outputs
```

### ğŸ“ˆ æ¶ˆèå®éªŒç»“æœå¯¹æ¯”

| exp                       |   BLEU4 |   ROUGE1 |   ROUGEL |
|:--------------------------|--------:|---------:|---------:|
| ablate_no_label_smoothing |       0 |     0.07 |     0.04 |
| ablate_no_posenc          |       0 |     2.78 |     2.19 |
| ablate_single_head        |       0 |     5.58 |     4.55 |
| baseline                  |       0 |    10.06 |     6.99 |

| æ¨¡å‹é…ç½® | BLEU-1 | BLEU-2 | ROUGE-1 (F1) | ROUGE-L (F1) |
|---------|:------:|:------:|:------------:|:------------:|
| **Baseline** | ~12 | ~0.02 | ~13 | ~11 |
| **w/o PosEnc** | â†“ | â†“ | â†“â†“â†“ | â†“â†“â†“ |
| **Single-Head** | â†“ | â†“ | â†“ | â†“ |
| **w/o Label Smoothing** | Loss â†“ but BLEU/ROUGE å´©æºƒ | å´©æºƒ | å´©æºƒ | å´©æºƒ |
â¸»

## ğŸ“œ å¼•ç”¨

ä¸»è¦å‚è€ƒçš„æ–‡ç« ï¼š

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and others},
  journal={NeurIPS},
  year={2017}
}
